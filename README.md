# Compress-BERT-to-a-Small-Version-with-3-or-6-Transformer-Encoder-Blocks.  
## Objective Function  
L = L_CE + L_KL + L_MSE.
## References  
EMNLP 2019 - Patient Knowledge Distillation for BERT Model Compression.  
NIPS 2014 - Distilling the knowledge in a neural network. 
